{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62031f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import sys\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import sounddevice as sd\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a76f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(img):\n",
    "    ''' Draw the landmarks of the detected hands on the given image,\n",
    "        and get the coordinates of the THUMP_TIP(4) and of the INDEX_TIP(8) of the one hand'''\n",
    " \n",
    "    coord_lms = []\n",
    "    height, width, _ = img.shape\n",
    "    indexes = [4, 8]\n",
    "    \n",
    "    # For using the Mediapipe hand tracking solution\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    \n",
    "    # Initialize the hand tracking model\n",
    "    hands = mp_hands.Hands()\n",
    "    results = hands.process(img)\n",
    "    \n",
    "    # The landmarks of the detected hands\n",
    "    landmarks = results.multi_hand_landmarks\n",
    "\n",
    "    # Draw hand landmarks of each hand\n",
    "    if landmarks:\n",
    "        # If landmarks are found, it iterates through each detected hand\n",
    "        for hand_landmarks in landmarks:\n",
    "            # draw the landmarks on the original image\n",
    "            mp_drawing.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "        # Get the coordinates of the THUMP_TIP and of the INDEX_TIP of the first hand\n",
    "        hand_lms = landmarks[0]\n",
    "        for index in indexes:\n",
    "            landmark = hand_lms.landmark[index]\n",
    "            width_lm = round(landmark.x * width)\n",
    "            height_lm = round(landmark.y * height)\n",
    "            coord_lms.append((width_lm, height_lm))\n",
    "                    \n",
    "    return coord_lms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e4c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(img, resize_ratio=1):\n",
    "    ''' resize a given image'''\n",
    "    height, width, _ = img.shape\n",
    "    new_height = int(height * resize_ratio)\n",
    "    new_width = int(width * resize_ratio)\n",
    "    resized_img = cv2.resize(img, (new_width, new_height))  \n",
    "    return resized_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5035c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the VideoCapture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the VideoCapture is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print('Error: Could not open the camera.')\n",
    "    sys.exit()\n",
    "\n",
    "# Capture frame from the camera, and process it\n",
    "while True:        \n",
    "    _ , frame = cap.read()\n",
    "    resize_ratio = 1\n",
    "    resized_img = resize_img(frame, resize_ratio)\n",
    "    coord_lms = get_landmarks(resized_img)\n",
    "    \n",
    "    # If there are hands on the actuel frame\n",
    "    if coord_lms != []:\n",
    "        thumb_tip, index_tip = coord_lms\n",
    "        height, width, _ = resized_img.shape\n",
    "        \n",
    "        # Draw a line between the thumb tip and the index tip\n",
    "        cv2.circle(resized_img, thumb_tip, 10, (255,0,255), cv2.FILLED)\n",
    "        cv2.circle(resized_img, index_tip, 10, (255,0,255), cv2.FILLED)\n",
    "        cv2.line(resized_img, thumb_tip, index_tip, (255,0,255), 4)\n",
    "\n",
    "        # Compute the distance between thumb tip and the index tip\n",
    "        dist = np.linalg.norm(np.array(thumb_tip) - np.array(index_tip))\n",
    "        dist = round(np.interp(dist, [5, 95*resize_ratio], [0, 100]))\n",
    "        h_full_rect = round(np.interp(dist, [0, 100], [height-100, 100]))\n",
    "        \n",
    "        target_volume = dist / 100\n",
    "        \n",
    "        # Control the system's volume\n",
    "        if platform.system() == 'Windows':\n",
    "            # Get the default audio device\n",
    "            devices = AudioUtilities.GetSpeakers()\n",
    "            interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "\n",
    "            # Create an instance of the audio endpoint volume interface\n",
    "            volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "            volume.SetMasterVolumeLevelScalar(target_volume, None)# Set the volume\n",
    "        else:\n",
    "            # If the OS is Linux or macOS\n",
    "            default_device = sd.query_devices().index\n",
    "            sd._alsa._setoutputmute(default_device, False)  # Unmute the output if muted\n",
    "            sd._alsa._setoutputvolume(default_device, target_volume)  # Set the volume  \n",
    "            \n",
    "        # Draw rectangles that represent the volume\n",
    "        cv2.rectangle(resized_img, (width-100, height-100), (width-60, 100), (255,0,0), 2)\n",
    "        cv2.putText(resized_img, f'{dist}%', (width-100, 95), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.rectangle(resized_img, (width-100, height-100), (width-60, h_full_rect), (0,255,0), cv2.FILLED)\n",
    "        \n",
    "    # Display the captured frame after proccesing\n",
    "    cv2.imshow('My Camera', resized_img)\n",
    "    \n",
    "    # Press 'q' key to quit the VideoCapture\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture object and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_kernel",
   "language": "python",
   "name": "tf_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
